{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9189309-f7b2-4345-acd3-7765320c9929",
   "metadata": {},
   "source": [
    "# Distractor Analysis\n",
    "\n",
    "Here, we use distractor analysis to investigate consistency wrt to 3 dimensions:\n",
    "\n",
    "1) Are LLMs consistent in the sense that they choose the same options humans choose?\n",
    "2) Are LLMs consistent in the sense that they once they found the correct answer, do they keep it when presented with incorrect options?\n",
    "3) Are LLM consistent in the sense that they not change their mind in non-logical way (choosing an option they could already have chosen), when presented with different subsets of options?\n",
    "\n",
    "Exam codes:\n",
    "\n",
    "## LC - Languages\n",
    "## CH - Humanities\n",
    "## CN - Natural Sciences\n",
    "## MT - Math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c7f4f32-0d24-46a6-aeef-ceca60a639af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# jupyter nbconvert --no-input --to pdf enem-llm-distractor-analysis.ipynb         \n",
    "palette = [\"red\", \"purple\", \"blue\", \"green\"]\n",
    "dic_exam_to_paper_name = {'CH': 'Humanities', 'MT': 'Math', 'CN': 'Natural Sciences', 'LC': 'Languages' }\n",
    "\n",
    "os.makedirs(\"figures/distractor-analysis\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c71e953b-a1e0-49b7-bad9-a885e79d68ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading... CH 2020 mistral simple-zero-shot\n",
      "Loading... CH 2021 mistral simple-zero-shot\n",
      "Loading... CH 2022 mistral simple-zero-shot\n",
      "Loading... MT 2020 mistral simple-zero-shot\n",
      "Loading... MT 2021 mistral simple-zero-shot\n",
      "Loading... MT 2022 mistral simple-zero-shot\n",
      "Loading... CN 2022 mistral simple-zero-shot\n",
      "Loading... CH 2020 llama2 simple-zero-shot\n",
      "Loading... CH 2021 llama2 simple-zero-shot\n",
      "Loading... CH 2022 llama2 simple-zero-shot\n"
     ]
    }
   ],
   "source": [
    "from read_functions import read_human_data\n",
    "from read_functions import read_llm_data\n",
    "\n",
    "filepath = \"C:/Users/pedro/Downloads/TRI/test_responses_llms/EXP\"\n",
    "dic_scores, dic_random_scores, dic_itens, dic_logs, dic_test_responses, dic_average_theta_by_ctt_score, dic_average_theta_by_ctt_random_score = read_llm_data(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9cdfb1-6b0e-4146-8612-4a5bac2887bf",
   "metadata": {},
   "source": [
    "In the ENEM data, we have the specific option chosen by each student for every item. For instance, student 33 may have chosen option A for item 1 and option E for item 2.\r\n",
    "\r\n",
    "Human students engage in a form of reasoning that guides their choice of the option they believe is more likely to be correct. Therefore, comparing the choices made by LLMs with those made by humans is a reasonable way to evaluate if LLMs are engaging in some form of reasoning, or at least behaving as if they are.\r\n",
    "\r\n",
    "We compare the option choices of humans with the option choices of LLMs as follows: for students in the top 25% of grades, for each item, we order the options (A, B, C, D, E) based on popularity. Note that the most popular option is not necessarily the correct one, although it often is.\r\n",
    "\r\n",
    "Then, for all option shuffles for each LLM, we create a histogram showing the percentage of choices LLMs make corresponding to the 1st, 2nd, 3rd, 4th, or 5th option chosen by the top 25% of humans.\r\n",
    "\r\n",
    "For Humanities, Mistral chooses the most popular options chosen by humans approximately 70% of the time and the 2nd option 10-20% of the time. It occasionally chooses the 3rd, 4th, and 5th options.\r\n",
    "\r\n",
    "We interpret this result as a signal that Mistral reasons similarly to humans: it typically chooses options that humans choose, even when it makes a mistake.\r\n",
    "\r\n",
    "For Natural Sciences, Mistral still chooses option 1 more frequently, but now only 45% of the time instead of 70%. As options become less preferred for humans, Mistral also shows less preference, but the correlation is less strong.\r\n",
    "\r\n",
    "In Math, it's important to note that the correlation is even weaker, signaling that Mistral's reasoning is not similar to humans while taking the Math test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4f5d98-0f08-4caa-a60c-95ef862abb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "for exam in ['CH', 'CN', 'MT', 'LC']:\n",
    "    for llm in ['mistral', 'llama2']:\n",
    "\n",
    "        if exam not in dic_logs[llm].keys() or len(dic_logs[llm][exam]) == 0:\n",
    "            continue\n",
    "        \n",
    "        human_sequence_data = []\n",
    "        year_data = []\n",
    "        for year in dic_logs[llm][exam].keys():\n",
    "                   sequence = dic_logs[llm][exam][year]['human_sequence'].tolist()\n",
    "\n",
    "                   # We do not want to take guessed options into account.\n",
    "                   if 'invalid_answer' in dic_logs[llm][exam][year].columns:\n",
    "                       invalid = dic_logs[llm][exam][year]['invalid_answer'].tolist()\n",
    "                       sequence = [num for num, invalid in zip(sequence, invalid) if invalid == False]\n",
    "                         \n",
    "                        \n",
    "                   human_sequence_data += sequence\n",
    "                   year_data += [year] * len(sequence)\n",
    "            \n",
    "        dfs = pd.DataFrame(data={'human_sequence': human_sequence_data, \n",
    "                             'year': year_data \n",
    "                        })\n",
    "\n",
    "        sns.histplot(data=dfs, x=\"human_sequence\", hue=\"year\", stat=\"probability\", common_norm=False, multiple=\"dodge\"), #shrink=.8)\n",
    "        \n",
    "        plt.xlabel('Chosen options - most preferred to least preferred by humans')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(f'test: {dic_exam_to_paper_name[exam]} - {llm}')\n",
    "        plt.xticks([1, 2, 3, 4, 5])\n",
    "        plt.savefig(f'figures/distractor-analysis/{llm}-{exam}-humanity-histogram.pdf', format='pdf',bbox_inches='tight')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfa2b41-1222-4aa5-a09c-2d9b55c3f0a4",
   "metadata": {},
   "source": [
    "Here we show the same data, but in a compressed form.\n",
    "\"Average Humanity\" is the average choice LLMs make, 1st to 5h option.\n",
    "For each option shuffling, we can compute the average humanity of the LLM for that particular shuffling, and hence we can display a boxplot.\n",
    "\n",
    "In the boxplot,  1 means the LLM always choose the options human prefer.\n",
    "Results show that LLM is very human in the Humanities test, less human in the Natural Sciences and even less in the Math.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7cdaba-244e-454c-9958-86b470793d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "for llm in ['mistral', 'llama2']:\n",
    "    dfs = []\n",
    "    for exam in dic_test_responses[llm].keys():\n",
    "        for year in dic_test_responses[llm][exam].keys():\n",
    "            dic_test_responses[llm][exam][year]['test'] = dic_exam_to_paper_name[exam]\n",
    "            dic_test_responses[llm][exam][year]['year'] = year\n",
    "            dfs.append(dic_test_responses[llm][exam][year])\n",
    "            \n",
    "    df = pd.concat(dfs, ignore_index=True, axis=0)\n",
    "    plt.title(f\"{llm}\")\n",
    "    sns.boxplot(data=df, y=\"AVG_HUMANITY\", x=\"test\", hue=\"year\", order=['Humanities', 'Languages', 'Natural Sciences', 'Math'])\n",
    "    plt.savefig(f'figures/distractor-analysis/avg-{llm}-humanity.pdf', format='pdf',bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b38306-be07-43cc-9421-c84bbeb3c04e",
   "metadata": {},
   "source": [
    "One distractor analysis costly to conduct with real students but that we can run with LLMs is to present differente subsets of the options in order to unveil consistency patterns.\n",
    "\n",
    "Suppose I ask you, 'What is the capital of Massachusetts?' If you know the answer or know how to arrive at it, it should not affect much whether I present you with 2 options, 5 options, or 20 options. Hence, the rate at which LLMs change their choice as we present more options and make the test monotonically harder unveils the degree to which the LLM possesses 'true knowledge.'\r\n",
    "\r\n",
    "To run fewer experiments, for each test and year, we conducted the following experiments: we presented the top two most popular options, then the top three, the top four, and only then the complete set of 5 options. The correct option is included as the first option regardless of its popularity. In a second experiment, we also present 2, 3, 4, and 5 options, but now we choose from the least chosen to the most chosen. For example, we expect this test to be easier for LLMs. Again, the correct option is always selected as the first option.\r\n",
    "\r\n",
    "The behavior is very different depending on the test. For humanities, when presented with two options, accuracy is around 80% â€” note that a random guesser would on average score 50%. The interesting pattern is that as we present more options to the LLM, accuracy is kept roughly at the same level, meaning that the LLM is not distracted by wrong options.\r\n",
    "\r\n",
    "For Natural Sciences, accuracy for two options starts at 73%, which is not that far from 80% for Humanities. However, as we present more options to the LLM, accuracy decreases and stops  when five options are presented.\n",
    "\n",
    "In this graph we show only mistral in PT-BR. I think that when we plot several LLMs with different setups, some curves will reach the same score @ 5 options, but the curve shape will be different.ons\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d9a0ea-8e09-4124-a0a5-4b18e72b3bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# TODO: guardar acuracia em test_responses, mais facil explicar no paper\n",
    "# TODO: guardar \"number_of_options\" instead of VALID_OPTION_COUNT\n",
    "\n",
    "llm = 'mistral'\n",
    "filepath = \"C:/Users/pedro/Downloads/TRI/test_responses_llms/SUBSET\"\n",
    "\n",
    "ch_df = pd.read_csv(f\"{filepath}/CH/pt-br/2022/mistral/simple-zero-shot/aggregated/test_responses.csv\")\n",
    "cn_df = pd.read_csv(f\"{filepath}/CN/pt-br/2022/mistral/simple-zero-shot/aggregated/test_responses.csv\")\n",
    "mt_df = pd.read_csv(f\"{filepath}/MT/pt-br/2022/mistral/simple-zero-shot/aggregated/test_responses.csv\")\n",
    "lc_df = pd.read_csv(f\"{filepath}/LC/pt-br/2022/mistral/simple-zero-shot/aggregated/test_responses.csv\")\n",
    "\n",
    "\n",
    "\n",
    "ch_df['accuracy'] = ch_df['CTT_SCORE'] / 45\n",
    "cn_df['accuracy'] = cn_df['CTT_SCORE'] / 45\n",
    "mt_df['accuracy'] = mt_df['CTT_SCORE'] / 45\n",
    "lc_df['accuracy'] = lc_df['CTT_SCORE'] / 45\n",
    "\n",
    "ch_df['expected_accuracy'] = ch_df['EXPECTED_CTT_SCORE'] / 45\n",
    "cn_df['expected_accuracy'] = cn_df['EXPECTED_CTT_SCORE'] / 45\n",
    "mt_df['expected_accuracy'] = mt_df['EXPECTED_CTT_SCORE'] / 45\n",
    "lc_df['expected_accuracy'] = lc_df['EXPECTED_CTT_SCORE'] / 45\n",
    "\n",
    "sns.lineplot(x=ch_df['VALID_OPTION_COUNT'], y=ch_df['accuracy'], label='Languages', marker='o', errorbar='ci')\n",
    "sns.lineplot(x=lc_df['VALID_OPTION_COUNT'], y=lc_df['accuracy'], label='Humanities', marker='o', errorbar='ci')\n",
    "sns.lineplot(x=cn_df['VALID_OPTION_COUNT'], y=cn_df['accuracy'], label='Natural Sciences', marker='o', errorbar='ci')\n",
    "#  BASELINE\n",
    "sns.lineplot(x=cn_df['VALID_OPTION_COUNT'], y=cn_df['expected_accuracy'], label='Random', marker='o', errorbar='ci')\n",
    "sns.lineplot(x=mt_df['VALID_OPTION_COUNT'], y=mt_df['accuracy'], label='Math', marker='o', errorbar='ci')\n",
    "\n",
    "\n",
    "\n",
    "#plt.xticks([1, 2, 3, 4, 5])\n",
    "plt.title('mistral')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('# of options')\n",
    "plt.xticks([2, 3, 4, 5])\n",
    "\n",
    "plt.savefig(f'figures/distractor-analysis/subset-accuracy-{llm}.pdf', format='pdf',bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68c8a6e-3525-432e-9f29-867745c571c3",
   "metadata": {},
   "source": [
    "Another distractor analysis we can conduct is to observe not only the rate at which the LLM change its mind\n",
    "when presented with an (incorrect) option it has not seen before, but also if the change is consistent.\n",
    "\n",
    "A consistent change happens when presented with an extra option, the LLM chooses it. For example, if when presented with AB the LLM chooses B,\n",
    "it is a consistent change to chose C when presented with ABC. However, sometimes a LLM changes its mind and changes its choice to an option\n",
    "it could have chosen already, for example, choosing B from AB but choosing A when presented with ABC. \n",
    "\n",
    "Again, mistral is quite consistent while doing the Humanities test: it keeps its original choice 85% of the time, and only 3% of the time it performs an inconsistent change. Again, Natural Sciences is between Humanities and Math."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b9ad18-d4b2-47ec-bae2-d54ad537bd4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "ch_df = pd.read_csv(f\"{filepath}/CH/pt-br/2022/mistral/simple-zero-shot/aggregated/subset_consistency.csv\")\n",
    "cn_df = pd.read_csv(f\"{filepath}/CN/pt-br/2022/mistral/simple-zero-shot/aggregated/subset_consistency.csv\")\n",
    "mt_df = pd.read_csv(f\"{filepath}/MT/pt-br/2022/mistral/simple-zero-shot/aggregated/subset_consistency.csv\")\n",
    "lc_df = pd.read_csv(f\"{filepath}/LC/pt-br/2022/mistral/simple-zero-shot/aggregated/subset_consistency.csv\")\n",
    "\n",
    "dic_subset_df = defaultdict(dict)\n",
    "\n",
    "dic_subset_df['mistral']['CH'] = {}\n",
    "dic_subset_df['mistral']['LC'] = {}\n",
    "dic_subset_df['mistral']['CN'] = {}\n",
    "dic_subset_df['mistral']['MT'] = {}\n",
    "\n",
    "dic_subset_df['mistral']['CH'][2022] = ch_df\n",
    "dic_subset_df['mistral']['LC'][2022] = lc_df\n",
    "dic_subset_df['mistral']['CN'][2022] = cn_df\n",
    "dic_subset_df['mistral']['MT'][2022] = mt_df\n",
    "\n",
    "\n",
    "\n",
    "categories = []\n",
    "percentages = []\n",
    "exams = []\n",
    "for llm in ['mistral']:\n",
    "    for exam in dic_subset_df[llm].keys():\n",
    "        for year in dic_subset_df[llm][exam]:\n",
    "            sum_df = dic_subset_df[llm][exam][year].sum()/dic_subset_df[llm][exam][year]['possible_changes'].sum()\n",
    "\n",
    "            categories.extend(['no changes', 'consistent changes', 'inconsistent changes'])\n",
    "            percentages.extend([sum_df['no_changes'], sum_df['consistent_changes'], sum_df['inconsistent_changes']])\n",
    "            exams.extend([f\"{dic_exam_to_paper_name[exam]} - {year}\"] * 3)\n",
    "\n",
    "                       \n",
    "df = pd.DataFrame(data={'type of change': categories, 'ratio': percentages, 'exam': exams})\n",
    "g= sns.catplot(data=df, x=\"type of change\", y=\"ratio\", hue=\"exam\", kind=\"bar\")\n",
    "g.set_xticklabels(rotation=20)\n",
    "plt.savefig(f'figures/distractor-analysis/subset-consistency-{llm}.pdf', format='pdf',bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a823af7b-a8e5-4589-aa4c-f08cf0f0081a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
