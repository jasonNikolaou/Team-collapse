{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce204321-9ac5-437e-8f6c-e4585c4901c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "from collections import defaultdict\n",
    "from scipy.stats import percentileofscore\n",
    "\n",
    "def generate_human_histogram_df(responses_df, PROVA, CO_PROVA, o, min_nota, max_nota):\n",
    "\n",
    "    #responses_df = responses_df[responses_df['CO_PROVA_'+PROVA] == CO_PROVA]\n",
    "   \n",
    "    itens_df = pd.read_csv(item_df_path, sep=';', encoding = \"latin\")\n",
    "    itens_df = itens_df[itens_df['SG_AREA'] == PROVA]\n",
    "    itens_df = itens_df[itens_df['TP_LINGUA'] != 0]\n",
    "    print('len_itens_df', len(itens_df))\n",
    "    \n",
    "    itens_df = itens_df.sort_values(by=['CO_PROVA','CO_POSICAO'], ascending=True) \n",
    "    \n",
    "    map_co_item_to_target_item_id = dict()\n",
    "    target_item_id = 0\n",
    "    for idx, item in itens_df.iterrows():\n",
    "        co_item = item['CO_ITEM']\n",
    "        co_posicao = item['CO_POSICAO']\n",
    "        co_prova = item['CO_PROVA']\n",
    "        gabarito = item['TX_GABARITO']\n",
    "\n",
    "        if co_prova == CO_PROVA:\n",
    "            target_item_id += 1\n",
    "            map_co_item_to_target_item_id[co_item] = target_item_id\n",
    "\n",
    "\n",
    "        #if co_item == 111664 and co_prova:\n",
    "        #    print('co_item', co_item, 'co_posicao', co_posicao, 'co_prova', co_prova, 'gabarito', gabarito, 'target_item_id', target_item_id)\n",
    "    \n",
    "    #print('map_co_item_to_target_item_id', map_co_item_to_target_item_id)\n",
    "    \n",
    "    map_co_prova_co_item_to_target_prova_item_id = defaultdict(dict)\n",
    "    map_co_prova_item_id_to_co_item = defaultdict(dict)\n",
    "    dic_item_id_count = defaultdict(int)\n",
    "    for idx, item in itens_df.iterrows():\n",
    "        co_item = item['CO_ITEM']\n",
    "        co_posicao = item['CO_POSICAO']\n",
    "        co_prova = item['CO_PROVA']\n",
    "        gabarito = item['TX_GABARITO']\n",
    "\n",
    "        dic_item_id_count[co_prova] += 1\n",
    "\n",
    "        if co_item not in map_co_item_to_target_item_id:\n",
    "            #print('co_item', co_item, 'not found in map_co_item_to_target_item_id')\n",
    "            continue\n",
    "\n",
    "        #print(co_prova, 'co_posicao', co_posicao, 'item_id', dic_item_id_count[co_prova], gabarito)\n",
    "        \n",
    "        #if co_item == 111664:\n",
    "        #    print('co_item', co_item, 'co_posicao', co_posicao, 'co_prova', co_prova, 'gabarito', gabarito, 'dic_item_id_count[co_prova]', dic_item_id_count[co_prova])\n",
    "        \n",
    "        map_co_prova_item_id_to_co_item[co_prova][dic_item_id_count[co_prova]]=co_item\n",
    "        map_co_prova_co_item_to_target_prova_item_id[co_prova][co_item] = map_co_item_to_target_item_id[co_item]\n",
    "\n",
    "        #print('co_prova', co_prova, 'co_posicao', co_posicao, 'item', dic_item_id_count[co_prova], 'is',co_item)\n",
    "\n",
    "    #print('map_co_prova_item_id_to_co_item contains PROVAS', map_co_prova_item_id_to_co_item.keys())\n",
    "    #for co_prova in map_co_prova_item_id_to_co_item.keys():\n",
    "        #print('prova ', co_prova, 'contains itens', map_co_prova_item_id_to_co_item[co_prova].keys())\n",
    "\n",
    "    # ACERTA ITENS_DF\n",
    "    itens_df = itens_df[itens_df['CO_PROVA'] == CO_PROVA].sort_values(by='CO_POSICAO', ascending=True)\n",
    "    itens_df.reset_index()\n",
    "    itens_df['human_item_id'] = range(1, len(itens_df) + 1)\n",
    "    \n",
    "    #print('len_responses_df', len(responses_df))\n",
    "    #print('map_co_prova_item_id_to_co_item', map_co_prova_item_id_to_co_item)\n",
    "    \n",
    "    options =  ['A', 'B', 'C', 'D', 'E']\n",
    "    \n",
    "    histograms_options = {}\n",
    "    histograms_correct = { }\n",
    "    correct_options = list()\n",
    "    correct_response_pattern = \"\"\n",
    "\n",
    "    student_count = 0\n",
    "\n",
    "    record_count = 0\n",
    "\n",
    "    filtered_responses_df = responses_df[(responses_df['NU_NOTA'] >= min_nota) & (responses_df['NU_NOTA'] <= max_nota)]\n",
    "    \n",
    "    dic_correct_option = dict()\n",
    "    for index, student in filtered_responses_df.iterrows():      \n",
    "        #if student['TP_PRESENCA_' + PROVA] != 1:\n",
    "        #    continue  \n",
    "    \n",
    "        co_prova = student['CO_PROVA']\n",
    "\n",
    "        if co_prova not in dic_item_id_count:\n",
    "            continue\n",
    "        \n",
    "        #if co_prova != CO_PROVA:\n",
    "        #    continue\n",
    "        \n",
    "        student_count += 1\n",
    "        #if student_count % 10000 == 0:\n",
    "       #     print('student_count', student_count)\n",
    "\n",
    "        #if student_count == 200000:\n",
    "        #    break\n",
    "\n",
    "        record_count += 1\n",
    "\n",
    "        for item_id, choice_original_order in enumerate(student['TX_RESPOSTAS'], start=1):\n",
    "            \n",
    "            #if co_prova != CO_PROVA:\n",
    "            #    continue\n",
    "            \n",
    "            #shuffled_response_pattern = item['SHUFFLED_ORDER_TX_RESPOSTAS']\n",
    "            response_pattern = student['TX_RESPOSTAS']\n",
    "            #shuffled_correct_response_pattern = item['SHUFFLED_ORDER_TX_GABARITO']\n",
    "            correct_response_pattern = student['TX_GABARITO']\n",
    "        \n",
    "            #choice_shuffled_order = shuffled_response_pattern[item_id-1]\n",
    "            #correct_shuffled_order = shuffled_correct_response_pattern[item_id-1]\n",
    "\n",
    "            choice_original_order = response_pattern[item_id-1]\n",
    "            correct_original_order = correct_response_pattern[item_id-1]\n",
    "\n",
    "            if item_id not in map_co_prova_item_id_to_co_item[co_prova]:\n",
    "                #print('item_id', item_id, 'not in co_prova', co_prova)\n",
    "                continue\n",
    "                \n",
    "            co_item = map_co_prova_item_id_to_co_item[co_prova][item_id]\n",
    "            target_item_id = map_co_prova_co_item_to_target_prova_item_id[co_prova][co_item]\n",
    "            correct = choice_original_order == correct_original_order\n",
    "\n",
    "            #if co_item == 111664:\n",
    "            #    print(\"Adding for co_prova\", co_prova, \"co_item\", co_item, \"item_id\", item_id, \"target_item_id\", target_item_id, \"correct\", correct_original_order)\n",
    "            #    print(\"AAAH! correct option for target_item_id \" + str(target_item_id) + \" differs. \" \\\n",
    "            #                    \"item_id is\",item_id, \\\n",
    "            #                    \"co_prova is\",co_prova,  \\\n",
    "            #                    \"gabarito is\", correct_response_pattern)\n",
    "            \n",
    "            if target_item_id in dic_correct_option and dic_correct_option[target_item_id] != correct_original_order:\n",
    "                raise Exception(\"Fatal error: correct option for target_item_id \" + str(target_item_id) + \" differs. \" \\\n",
    "                                \"item_id is\",item_id,\n",
    "                                \"co_prova is\",co_prova,  \\\n",
    "                                \"gabarito is\", correct_response_pattern, \\\n",
    "                                \"It is \" + dic_correct_option[target_item_id] + \"but we're trying to change to \" + correct_original_order)\n",
    "\n",
    "            dic_correct_option[target_item_id] = correct_original_order\n",
    "            \n",
    "            # Update the histogram for the current position and letter\n",
    "            if target_item_id not in histograms_options:\n",
    "                histograms_options[target_item_id] = { }\n",
    "                for option in options:\n",
    "                    histograms_options[target_item_id][option] = 0\n",
    "                histograms_correct[target_item_id] = { }\n",
    "\n",
    "                        \n",
    "            if o == 'choice_original_order':\n",
    "                letter = choice_original_order\n",
    "                if letter not in options:\n",
    "                    continue\n",
    "                histograms_options[target_item_id][letter] = histograms_options[target_item_id].get(letter, 0) + 1\n",
    "                          \n",
    "            elif o == 'choice_shuffled_order':\n",
    "                letter = choice_shuffled_order\n",
    "                histograms_options[target_item_id][letter] = histograms_options[target_item_id].get(letter, 0) + 1\n",
    "            \n",
    "            if choice_original_order == correct_original_order:\n",
    "                    column_name = 'human_accuracy'\n",
    "                    histograms_correct[target_item_id][column_name] = histograms_correct[target_item_id].get(column_name, 0) + 1\n",
    "            else:\n",
    "                    column_name = 'WRONG'\n",
    "                    histograms_correct[target_item_id][column_name] = histograms_correct[target_item_id].get(column_name, 0) + 1    \n",
    "\n",
    "    print('record_count', record_count)\n",
    "    #print('histogram_correct')\n",
    "    #for key in  sorted(histograms_correct.keys()):\n",
    "    #    print(f'{key}: {histograms_correct[key]}')\n",
    "\n",
    "    #print('dict_correct_option')\n",
    "    #for key in  sorted(dic_correct_option.keys()):\n",
    "    #    print(f'{key}: {dic_correct_option[key]}')\n",
    "\n",
    "\n",
    "    #print('histogram_options', histograms_options)\n",
    "    # Convert the histograms to a DataFrame for better presentation\n",
    "    histogram_options_df = pd.DataFrame(histograms_options).sort_index()\n",
    "    histogram_options_df.fillna(0, inplace=True)\n",
    "    histogram_options_df = histogram_options_df.sort_index(axis=1)\n",
    "\n",
    "    histogram_correct_df = pd.DataFrame(histograms_correct).sort_index()\n",
    "    histogram_correct_df.fillna(0, inplace=True)\n",
    "    histogram_correct_df = histogram_correct_df.sort_index(axis=1)\n",
    "\n",
    "    # Display the resulting histogram DataFrame\n",
    "    normalized_histogram_options_df = histogram_options_df/histogram_options_df.sum()\n",
    "    normalized_histogram_options_df = normalized_histogram_options_df.transpose()\n",
    "    normalized_histogram_options_df['human_entropy'] = normalized_histogram_options_df.apply(entropy, axis=1) # +  np.random.uniform(-0.001, 0.001, len(normalized_histogram_df['A']))  \n",
    "    normalized_histogram_options_df['human_item_id'] = range(1, len(normalized_histogram_options_df) + 1)\n",
    "\n",
    "    normalized_histogram_correct_df = histogram_correct_df/histogram_correct_df.sum()\n",
    "    normalized_histogram_correct_df = normalized_histogram_correct_df.transpose()\n",
    "    normalized_histogram_correct_df['human_item_id'] = range(1, len(normalized_histogram_correct_df) + 1)\n",
    "   \n",
    "    normalized_histogram_options_df['CORRECT_OPTION_HUMAN'] =  [value for key, value in sorted(dic_correct_option.items())]\n",
    "    histogram_df = pd.merge(normalized_histogram_options_df, normalized_histogram_correct_df, how='left', on='human_item_id')\n",
    "    return pd.merge(histogram_df, itens_df, how='left', on='human_item_id')\n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01b11c04-71c9-4c6b-a768-d4b8ea88098a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generated_sorted_letters_table(results_df, filepath):\n",
    "\n",
    "    co_posicao_items = list()\n",
    "    ordered_letters_easy_to_hard = list()\n",
    "    ordered_letters_hard_to_easy = list()\n",
    "    correct_options = list()\n",
    "    for idx, item in results_df.iterrows():\n",
    "        dic = dict()\n",
    "\n",
    "        co_posicao_items.append(item['CO_POSICAO'])\n",
    "        correct_options.append(item['CORRECT_OPTION_HUMAN'])\n",
    "    \n",
    "        dic['A'] = item['human_A_75_100']\n",
    "        dic['B'] = item['human_B_75_100']\n",
    "        dic['C'] = item['human_C_75_100']\n",
    "        dic['D'] = item['human_D_75_100']\n",
    "        dic['E'] = item['human_E_75_100']\n",
    "\n",
    "        dic = dict(sorted(dic.items(), key=lambda item: item[1], reverse=False))\n",
    "        letter_order = item['CORRECT_OPTION_HUMAN']\n",
    "        for letter in dic.keys():\n",
    "            if letter not in letter_order:\n",
    "                letter_order += letter\n",
    "        ordered_letters_easy_to_hard.append(letter_order)\n",
    "\n",
    "        dic = dict(sorted(dic.items(), key=lambda item: item[1], reverse=True))\n",
    "        letter_order = item['CORRECT_OPTION_HUMAN']\n",
    "        for letter in dic.keys():\n",
    "            if letter not in letter_order:\n",
    "                letter_order += letter\n",
    "        ordered_letters_hard_to_easy.append(letter_order)\n",
    "\n",
    "    df = pd.DataFrame({'CO_POSICAO': co_posicao_items, 'ORDERED_OPTIONS_HARD_TO_EASY': ordered_letters_hard_to_easy, 'ORDER_OPTIONS_EASY_TO_HARD': ordered_letters_easy_to_hard, 'CORRECT_OPTION': correct_options})\n",
    "\n",
    "    df.to_csv(filepath)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f5f0611-8582-4a0b-9038-3080ecde6232",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading enem... (2022, 'CH', 1062)\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 43\u001b[0m\n\u001b[0;32m     39\u001b[0m co_prova \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     41\u001b[0m columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCO_PROVA_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mPROVA,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNU_NOTA_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mPROVA,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTX_RESPOSTAS_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mPROVA,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTX_GABARITO_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mPROVA, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTP_PRESENCA_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mPROVA]\n\u001b[1;32m---> 43\u001b[0m enem_human_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:/Users/pedro/Downloads/TRI/test_responses_humans/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43myear\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/responses/test_responses_humans_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mPROVA\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43myear\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m enem_human_df\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNU_NOTA\u001b[39m\u001b[38;5;124m'\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m#enem_human_df = pd.read_csv(f\"C:/Users/pedro/Downloads/TRI/microdados/microdados_enem_{year}/DADOS/MICRODADOS_ENEM_{year}.csv\", usecols=columns, sep=\";\",  encoding='latin') \u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m     (\n\u001b[0;32m   1745\u001b[0m         index,\n\u001b[0;32m   1746\u001b[0m         columns,\n\u001b[0;32m   1747\u001b[0m         col_dict,\n\u001b[1;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:904\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2058\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 2022\n",
    "# CH 1062 \n",
    "# MT 1082\n",
    "# CN 1092\n",
    "\n",
    "# 2021\n",
    "# CH 886\n",
    "# CN 916\n",
    "# MT 906\n",
    "\n",
    "# 2020\n",
    "# CH 574\n",
    "# CN 604\n",
    "# MT 594\n",
    "\n",
    "# 2019\n",
    "# CH 520\n",
    "# MT 522\n",
    "# CN 519\n",
    "\n",
    "# 2018\n",
    "# CH 464\n",
    "# MT 466\n",
    "\n",
    "ch_tuples = [(2022, 'CH', 1062), (2021, 'CH', 886), (2020, 'CH', 574), (2019, 'CH', 520), (2018, 'CH', 464)]\n",
    "cn_tuples = [(2022, 'CN', 1092), (2021, 'CN', 916), (2020, 'CN', 604), (2019, 'CN', 519), (2018, 'CN', 463)]\n",
    "mt_tuples = [(2022, 'MT', 1082), (2021, 'MT', 906), (2020, 'MT', 594), (2019, 'MT', 522), (2018, 'MT', 466)]\n",
    "lc_tuples = [(2022, 'LC', 1072), (2021, 'LC', 896), (2020, 'LC', 584), (2019, 'LC', 521)]\n",
    "\n",
    "\n",
    "for tuple in ch_tuples + cn_tuples + mt_tuples + lc_tuples:\n",
    "    print('reading enem...', tuple)\n",
    "    \n",
    "    year = tuple[0]\n",
    "    PROVA = tuple[1]\n",
    "    co_prova = tuple[2]\n",
    "    \n",
    "    columns = ['CO_PROVA_'+PROVA,'NU_NOTA_'+PROVA,'TX_RESPOSTAS_'+PROVA,'TX_GABARITO_'+PROVA, 'TP_PRESENCA_'+PROVA]\n",
    "\n",
    "    enem_human_df = pd.read_csv(f\"C:/Users/pedro/Downloads/TRI/test_responses_humans/{year}/responses/test_responses_humans_{PROVA}_{year}.csv\")\n",
    "    enem_human_df.sort_values(by='NU_NOTA', inplace=True, ascending=False)\n",
    "\n",
    "    #enem_human_df = pd.read_csv(f\"C:/Users/pedro/Downloads/TRI/microdados/microdados_enem_{year}/DADOS/MICRODADOS_ENEM_{year}.csv\", usecols=columns, sep=\";\",  encoding='latin') \n",
    "    item_df_path = f\"c:/Users/pedro/Downloads/TRI/microdados/microdados_enem_{year}/DADOS/ITENS_PROVA_{year}.csv\"\n",
    "\n",
    "    print('enem read.')\n",
    "    percentiles = [0, 0.25, 0.50, 0.75, 1.0]\n",
    "    str_percentiles = [0, 25, 50, 75, 100]\n",
    "    #percentiles = [0,1]\n",
    "    #percentiles = [0, 50, 100]\n",
    "    nota_percentis = []\n",
    "    percentil_idx = 0\n",
    "    for p in percentiles:\n",
    "        nota = enem_human_df['NU_NOTA'].quantile(p)\n",
    "        print('nota', nota)\n",
    "        nota_percentis.append(nota)\n",
    "\n",
    "    print('grouping...')\n",
    "    grouped = enem_human_df.groupby('CO_PROVA')['NU_NOTA'].count().sort_values(ascending=False)\n",
    "    print(grouped)\n",
    "\n",
    "    co_provas = enem_human_df['CO_PROVA'].unique()\n",
    "    enem_human_df.sort_values(by='NU_NOTA', inplace=True, ascending=False)\n",
    "\n",
    "    all_dfs = []\n",
    "    for id_percentil in range(0, len(nota_percentis) - 1):\n",
    "         min_nota = nota_percentis[id_percentil]\n",
    "         max_nota = nota_percentis[id_percentil+1]\n",
    "         string_pct = str(str_percentiles[id_percentil]) + \"_\" + str(str_percentiles[id_percentil+1])\n",
    "            \n",
    "         print(min_nota, max_nota)   \n",
    "         df = generate_human_histogram_df(enem_human_df, PROVA=PROVA, CO_PROVA=co_prova, o='choice_original_order', min_nota=min_nota,max_nota=max_nota)\n",
    "         df.rename(columns={'A': 'human_A_'+string_pct, 'B': 'human_B_'+string_pct, 'C': 'human_C_'+string_pct, 'D': 'human_D_'+string_pct, 'E': 'human_E_'+string_pct}, inplace=True)\n",
    "         df.rename(columns={'human_accuracy':'human_accuracy_'+string_pct},inplace=True)\n",
    "         df.rename(columns={'human_entropy':'human_entropy_'+string_pct},inplace=True)\n",
    "\n",
    "         if 'human_A_75_100' not in df.columns:\n",
    "               df = df.drop('CORRECT_OPTION_HUMAN', axis=1)\n",
    "               df = df.drop('CO_POSICAO', axis=1)\n",
    "               df = df.drop('CO_HABILIDADE', axis=1)\n",
    "               df = df.drop('CO_ITEM', axis=1)\n",
    "               df = df.drop('NU_PARAM_A', axis=1)\n",
    "               df = df.drop('NU_PARAM_B', axis=1)\n",
    "               df = df.drop('NU_PARAM_C', axis=1)\n",
    "               df = df.drop('TX_MOTIVO_ABAN', axis=1)\n",
    "               df = df.drop('TP_LINGUA', axis=1)\n",
    "               df = df.drop('TX_COR', axis=1)\n",
    "\n",
    "        \n",
    "         if 'WRONG' in df.columns:\n",
    "               df.drop(columns='WRONG', inplace=True)\n",
    "         all_dfs.append(df)\n",
    "         filename = f\"C:/Users/pedro/Downloads/TRI/test_responses_humans/{year}/itens/intermediarios/human_itens_{PROVA}_{int(co_prova)}_{year}_{string_pct}.csv\"\n",
    "         print(\"SAVING\", filename, df.columns)\n",
    "         df.to_csv(filename)\n",
    "        \n",
    "    result_df = pd.concat(all_dfs, axis=1, verify_integrity=False)\n",
    "    print('final columns', sorted(result_df.columns))\n",
    "    result_df.to_csv(f\"C:/Users/pedro/Downloads/TRI/test_responses_humans/{year}/itens/human_itens_{PROVA}_{int(co_prova)}_{year}.csv\")\n",
    "\n",
    "    generated_sorted_letters_table(result_df, f\"C:/Users/pedro/Downloads/TRI/test_responses_humans/{year}/itens/human_itens_ordered_options_{PROVA}_{int(co_prova)}_{year}.csv\")\n",
    "\n",
    "        #df = generate_human_histogram_df(enem_human_df, PROVA=exam, CO_PROVA=co_prova, o='correct_percentage', normalize=True)\n",
    "        #df.to_csv(f\"C:/Users/pedro/Downloads/TRI/test_responses_humans/2022/correct_percentage/correct_percentage_{exam}_{int(co_prova)}.csv\")\n",
    "\n",
    "print(\"FINISHED.\")     \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
